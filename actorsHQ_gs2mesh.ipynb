{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34078130-f07c-4e40-b709-615cc968cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  IMPORTANT: \n",
    "# =============================================================================\n",
    "# This notebook is a modified replica of the custom_data notebook provided in\n",
    "# the gs2mesh repository. In order to make variable image sizes work, we have \n",
    "# implemented patches to the intermediate steps and outputs of gs2mesh. \n",
    "\n",
    "# DO NOT change the order of the cells. The functionality is dependent on it.\n",
    "# DO NOT edit in places that you are not prompted to edit.\n",
    "# PLEASE KEEP args.skip_video_extraction and args.skip_colmap set to True, and \n",
    "# follow the guidelines provided for data population and COLMAP reconstruction \n",
    "# in our repository.\n",
    "\n",
    "# Please note:\n",
    "# If the cleaned tsdf mesh saving throws an error saying PLY could not be saved\n",
    "# because it has 0 vertices, please adjust the args.TSDF_cleaning_threshold \n",
    "# parameter. For smaller garments such as shorts, a 10x reduction from default\n",
    "# value is sufficient.\n",
    "# =============================================================================\n",
    "#  Imports\n",
    "# =============================================================================\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shutil\n",
    "import types\n",
    "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook' # changed from iframe\n",
    "\n",
    "import open3d as o3d\n",
    "import sys\n",
    "gs2mesh_path = os.path.join(os.getcwd(), \"gs2mesh\")\n",
    "\n",
    "# Changes the working directory to gs2mesh\n",
    "os.chdir(gs2mesh_path)\n",
    "\n",
    "# Adds gs2mesh to sys.path so Python can find modules inside it\n",
    "sys.path.insert(0, gs2mesh_path)\n",
    "print(\"Changed working directory to:\", os.getcwd())\n",
    "\n",
    "from gs2mesh_utils.argument_utils import ArgParser\n",
    "from gs2mesh_utils.colmap_utils import extract_frames, create_downsampled_colmap_dir, run_colmap, visualize_colmap_poses\n",
    "from gs2mesh_utils.eval_utils import create_strings\n",
    "from gs2mesh_utils.renderer_utils import Renderer\n",
    "from gs2mesh_utils.stereo_utils import Stereo\n",
    "from gs2mesh_utils.tsdf_utils import TSDF\n",
    "from gs2mesh_utils.masker_utils import init_predictor, Masker\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_dir = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0498e8-b3f7-431d-b7f3-7e5db4c6cd96",
   "metadata": {},
   "source": [
    "**Parameters:** (edit only here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa2061-eb8d-449b-b938-d4c2c9b5bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Initialize argument parser - DO NOT EDIT!\n",
    "# =============================================================================\n",
    "# Create argument parser with default arguments\n",
    "args = ArgParser('custom')\n",
    "\n",
    "# =============================================================================\n",
    "#  Parameters - EDIT ONLY HERE!\n",
    "# =============================================================================\n",
    "# please specify the garment to be processed, must be one of [upper, lower, dress]\n",
    "# where upper corresponds to tops, sweaters, jackets, etc.\n",
    "# lower corresponds to pants, shorts, etc., and dress is self-explanatory.\n",
    "garment_type = 'lower'\n",
    "\n",
    "# the absolute path to the directory where you require the final (cleaned) garment \n",
    "# mesh .obj is to be stored. \n",
    "mesh_output_path = '/home/hramzan/Desktop/semester-project/Gaussian-Garments/data/outputs/actor02_seq1'\n",
    "\n",
    "# General params\n",
    "args.dataset_name = 'custom' # Name of the dataset\n",
    "args.colmap_name = 'actor02_seq1' # Name of the directory with the COLMAP sparse model\n",
    "args.experiment_folder_name = None # Name of the experiment folder\n",
    "\n",
    "# Preprocessing params\n",
    "args.downsample = 1 # Downsampling factor\n",
    "\n",
    "# Gaussian Splatting parameters\n",
    "args.GS_iterations = 30000  # Number of Gaussian Splatting iterations\n",
    "args.GS_save_test_iterations = [7000, 30000]  # Gaussian Splatting test iterations to save\n",
    "args.GS_white_background = False  # Use white background in Gaussian Splatting\n",
    "\n",
    "# Renderer parameters\n",
    "args.renderer_baseline_absolute = None  # Absolute value of the renderer baseline (None uses 7 percent of scene radius)\n",
    "args.renderer_baseline_percentage = 7.0  # Percentage value of the renderer baseline\n",
    "args.renderer_scene_360 = True # Scene is a 360 scene\n",
    "args.renderer_folder_name = None  # Name of the renderer folder (None uses the colmap name)\n",
    "args.renderer_save_json = True  # Save renderer data to JSON\n",
    "args.renderer_sort_cameras = False  # Sort cameras in the renderer (True if using unordered set of views)\n",
    "\n",
    "# Stereo parameters\n",
    "args.stereo_model = 'DLNR_Middlebury'  # Stereo model to use\n",
    "args.stereo_occlusion_threshold = 3  # Occlusion threshold for stereo model (Lower value masks out more areas)\n",
    "args.stereo_shading_eps = 1e-4 # Small value used for visualization of the depth gradient. Adjusted according to the scale of the scene.\n",
    "args.stereo_warm = False  # Use the previous disparity as initial disparity for current view (False if views are not sorted)\n",
    "\n",
    "args.masker_automask = True # Use GroundingDINO for automatic object detection for masking with SAM2\n",
    "args.masker_prompt = 'beige shorts' # Prompt for GroundingDINO\n",
    "args.masker_SAM2_local = False # Use local SAM2 weights\n",
    "\n",
    "# TSDF parameters\n",
    "args.TSDF_scale = 1.0  # Fix depth scale\n",
    "args.TSDF_dilate = 1  # Take every n-th image (1 to take all images)\n",
    "args.TSDF_valid = None  # Choose valid images as a list of indices (None to ignore)\n",
    "args.TSDF_skip = None  # Choose non-valid images as a list of indices (None to ignore)\n",
    "args.TSDF_use_occlusion_mask = True  # Ignore occluded regions in stereo pairs for better geometric consistency\n",
    "args.TSDF_use_mask = False  # Use object masks (optional)\n",
    "args.TSDF_invert_mask = False  # Invert the background mask for TSDF. Only if TSDF_use_mask is True\n",
    "args.TSDF_erode_mask = True  # Erode masks in TSDF. Only if TSDF_use_mask is True\n",
    "args.TSDF_erosion_kernel_size = 10  # Erosion kernel size in TSDF.  Only if TSDF_use_mask is True\n",
    "args.TSDF_closing_kernel_size = 10  # Closing kernel size in TSDF.  Only if TSDF_use_mask is True.\n",
    "args.TSDF_voxel = 2  # Voxel size (voxel length is TSDF_voxel/512)\n",
    "args.TSDF_sdf_trunc = 0.04  # SDF truncation in TSDF\n",
    "args.TSDF_min_depth_baselines = 4  # Minimum depth baselines in TSDF\n",
    "args.TSDF_max_depth_baselines = 20  # Maximum depth baselines in TSDF\n",
    "args.TSDF_cleaning_threshold = 100000  # Minimal cluster size for clean mesh\n",
    "\n",
    "# Running parameters\n",
    "args.video_extension = 'mp4'  # Video file extension\n",
    "args.video_interval = 10  # Extract every n-th frame - aim for 3fps\n",
    "args.GS_port = 8090  # GS port number (relevant if running several instances at the same time)\n",
    "args.skip_video_extraction = True  # Skip the video extraction stage\n",
    "args.skip_colmap = True  # Skip the COLMAP stage\n",
    "args.skip_GS = True  # Skip the GS stage\n",
    "args.skip_rendering = True  # Skip the rendering stage\n",
    "args.skip_masking = True  # Skip the masking stage\n",
    "args.skip_TSDF = False  # Skip the TSDF stage\n",
    "\n",
    "# =============================================================================\n",
    "#  DO NOT EDIT THESE LINES:\n",
    "# =============================================================================\n",
    "colmap_dir = os.path.abspath(os.path.join(base_dir,'data', args.dataset_name, args.colmap_name))\n",
    "strings = create_strings(args)\n",
    "\n",
    "def set_references(garment_type):\n",
    "    if garment_type.lower() == \"upper\":\n",
    "        hor_ref, vert_ref = 1, 0\n",
    "    elif garment_type.lower() in [\"lower\", \"dress\"]:\n",
    "        hor_ref, vert_ref = 0, 0\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid garment type: {garment_type}\")\n",
    "    return hor_ref, vert_ref\n",
    "\n",
    "h_ref, v_ref = set_references(garment_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb33c59b-9e39-44eb-8202-fbdceaf12f03",
   "metadata": {},
   "source": [
    "**Extract frames if needed and Run COLMAP:** (only run if you don't have a COLMAP dataset. If you do, copy the colmap dataset to the \"data\" folder in the main root and update \"colmap_output_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381bc958-bb4f-4961-8fe6-c82f79c270e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Visualize the sparse COLMAP output and the COLMAP poses.\n",
    "# =============================================================================\n",
    "GT_path = None # OPTIONAL: compare to a GT point cloud if it is aligned with the COLMAP sparse point cloud\n",
    "# if you don't see the cameras, adjust the depth scale. If you don't see the points, adjust the subsample\n",
    "visualize_colmap_poses(colmap_dir, depth_scale=10.0, subsample=100, visualize_points=True, GT_path=GT_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446679e-ae19-48d9-946f-923a052b4cdb",
   "metadata": {},
   "source": [
    "**Run Gaussian Splatting:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651f6c7-299f-4025-b76f-11606243da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Run Gaussian Splatting\n",
    "# =============================================================================\n",
    "if not args.skip_GS:\n",
    "    try:\n",
    "        os.chdir(os.path.join(base_dir, 'third_party', 'gaussian-splatting'))\n",
    "        iterations_str = ' '.join([str(iteration) for iteration in args.GS_save_test_iterations])\n",
    "        os.system(f\"python train.py -s {colmap_dir} --port {args.GS_port} --model_path {os.path.join(base_dir, 'splatting_output', strings['splatting'], args.colmap_name)} --iterations {args.GS_iterations} --test_iterations {iterations_str} --save_iterations {iterations_str}{' --white_background' if args.GS_white_background else ''}\")\n",
    "        os.chdir(base_dir)\n",
    "    except:\n",
    "        os.chdir(base_dir)\n",
    "        print(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e97686-2c73-4301-8043-066aa0e5dfdc",
   "metadata": {},
   "source": [
    "**Prepare GS renderer for rendering stereo views:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade67e5-fa33-482b-a722-8444c291a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Initialize renderer\n",
    "# =============================================================================\n",
    "renderer = Renderer(base_dir, \n",
    "                    colmap_dir,\n",
    "                    strings['output_dir_root'],\n",
    "                    args,\n",
    "                    dataset = strings['dataset'], \n",
    "                    splatting = strings['splatting'],\n",
    "                    experiment_name = strings['experiment_name'],\n",
    "                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7fa6c-b076-4bab-99b6-8fb5357d89a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Visualize GS point cloud with COLMAP poses\n",
    "# =============================================================================\n",
    "# Green points are inside the FOV of at least one camera, given the min/max depth truncation at the TSDF stage.\n",
    "# Make sure that the object you want to reconstruct is Green. If not, adjust TSDF_max_depth_baselines to include the object.\n",
    "# If too much background is also green, reduce TSDF_max_depth_baselines to discard it.\n",
    "renderer.visualize_poses(depth_scale=10, subsample=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52e455-93ec-492b-a909-fd12cc40b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Prepare renderer\n",
    "# =============================================================================\n",
    "# ONLY NEED TO RUN ONCE PER SCENE!! Initializes renderer, takes some time\n",
    "if not args.skip_rendering:\n",
    "    renderer.prepare_renderer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f8f924-f19c-4930-b698-df76f1fe07f9",
   "metadata": {},
   "source": [
    "**Run Rendering + Stereo Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76d02e-8f99-4efa-ba3b-23d35488bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Initialize stereo\n",
    "# =============================================================================\n",
    "stereo = Stereo(base_dir, renderer, args, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adad24c-bdf6-4fa7-880f-181b3f27973c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Run stereo\n",
    "# =============================================================================\n",
    "%matplotlib inline\n",
    "if not args.skip_rendering:\n",
    "    stereo.run(start=0, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2dce6-93fe-4f2e-b6ea-35e642adc60c",
   "metadata": {},
   "source": [
    "**Run SAM2 Masker (NECESSARY FOR GARMENT SEGMENTATION):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4f347-1389-4753-b7e9-f4c4e01b871a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Initialize SAM2 predictor + GroundingDINO model\n",
    "# =============================================================================\n",
    "# ONLY NEED TO RUN ONCE PER SCENE!! Initializes SAM2 predictor and GroundingDino model, takes some time\n",
    "if not args.skip_masking:\n",
    "    GD_model, predictor, inference_state, images_dir = init_predictor(base_dir, renderer, args, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c3b57-d3f3-4281-99bc-1113766b752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Organises rendered images by sizes, since actorsHQ has hortizontal and vertical frames\n",
    "# =============================================================================\n",
    "if not args.skip_masking:\n",
    "    subdir_bins = []\n",
    "    for img in os.listdir(images_dir):\n",
    "        if os.path.splitext(img)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]:\n",
    "            src_im = os.path.join(images_dir, img)\n",
    "            image = plt.imread(src_im)\n",
    "            height, width = image.shape[:2]\n",
    "            subdir_bin = str(height) + 'x' + str(width)\n",
    "            if subdir_bin not in subdir_bins:\n",
    "                subdir_bins.append(subdir_bin)\n",
    "            os.makedirs(os.path.join(images_dir, subdir_bin), exist_ok=True)\n",
    "            shutil.copy(src_im, os.path.join(images_dir, subdir_bin, img))\n",
    "    \n",
    "    garment_dict = {\n",
    "            \"vertical\": {\"subdir\": next(dim for dim in subdir_bins if dim.startswith(\"1022\")), \"ref\": v_ref},\n",
    "            \"horizontal\": {\"subdir\": next(dim for dim in subdir_bins if dim.startswith(\"747\")), \"ref\": h_ref},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33823c3f-4170-4bb3-866d-8ff7d7da17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Patch on Masker class to handle variable size images data logic\n",
    "# =============================================================================\n",
    "def segment_custom(self, images_dir):\n",
    "    image_filenames = [\n",
    "            int(p.split('.')[0]) for p in os.listdir(images_dir)\n",
    "            if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "    ]\n",
    "    image_filenames.sort()\n",
    "    \n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in self.predictor.propagate_in_video(self.inference_state):\n",
    "        mask = (out_mask_logits[0] > 0.0).cpu().numpy().squeeze(0)\n",
    "        idx = image_filenames[out_frame_idx]\n",
    "        output_dir = self.renderer.render_folder_name(idx)\n",
    "        np.save(os.path.join(output_dir, 'left_mask.npy'), mask)\n",
    "        plt.imsave(os.path.join(output_dir, 'left_mask.png'), mask)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7e63b-d5c8-4388-a60e-2d14736b31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_masker_init(orientation):\n",
    "    subdir_bin = garment_dict[orientation][\"subdir\"]\n",
    "    predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-large\", device=device)\n",
    "    images_subdir = os.path.join(images_dir, subdir_bin)\n",
    "    inference_state = predictor.init_state(video_path=images_subdir)\n",
    "    \n",
    "    masker = Masker(GD_model, predictor, inference_state, images_subdir, renderer, stereo, args, image_number=garment_dict[orientation][\"ref\"], visualize=True)\n",
    "    return masker, images_subdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdbd3df-a78d-4456-90a0-c9c68333b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "if not args.skip_masking:\n",
    "    masker, images_subdir = view_masker_init(\"horizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ac7ca-5d34-4c13-be0a-ec8952b0230b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not args.skip_masking:\n",
    "    masker.segment_custom = types.MethodType(segment_custom, masker)\n",
    "    masker.segment_custom(images_subdir)\n",
    "\n",
    "    # once masks are generated, we need to store the first frame outputs \n",
    "    # to handle overwrite issue by moving the contents of 000 folder to 000_save\n",
    "    copy_src = renderer.render_folder_name(0)\n",
    "    copy_dest = copy_src + '_save'\n",
    "    \n",
    "    os.system(f\"mkdir {copy_dest}\")\n",
    "    os.system(f\"mv {copy_src + '/*'} {copy_dest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e14d4-1b1f-4ac7-b9d3-a621fbb22fc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "if not args.skip_masking:\n",
    "    masker, images_subdir = view_masker_init(\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a5c341-aeca-451f-82f1-cb498ee284a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.skip_masking:\n",
    "    masker.segment_custom = types.MethodType(segment_custom, masker)\n",
    "    masker.segment_custom(images_subdir)\n",
    "\n",
    "    # now, repopulate the original 000 folder with the stored data\n",
    "    os.system(f\"rm -rf {copy_src + '/*'}\")\n",
    "    os.system(f\"mv {copy_dest + '/*'} {copy_src}\")\n",
    "    os.system(f\"rm -rf {copy_dest}\")\n",
    "\n",
    "    # if the garment is an upper, then the 000 folder does not have any mask\n",
    "    # since it is skipped in masking. so, a manual copy paste from one of the \n",
    "    # later empty frames is done\n",
    "    if garment_type.lower() == 'upper':\n",
    "        src_path = renderer.render_folder_name(8)\n",
    "        dest_path = renderer.render_folder_name(0)\n",
    "        shutil.copy(os.path.join(src_path, 'left_mask.png'), os.path.join(dest_path, 'left_mask.png'))\n",
    "        shutil.copy(os.path.join(src_path, 'left_mask.npy'), os.path.join(dest_path, 'left_mask.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3169314c-1671-44ce-be32-88ac0b515187",
   "metadata": {},
   "source": [
    "**View Results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea19e3-0548-41ee-b969-77a5db8a1383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====================================================================================================\n",
    "#  View left-right renders, segmentation mask, disparity, occlusion mask and shading (depth gradient)\n",
    "# ====================================================================================================\n",
    "%matplotlib inline\n",
    "stereo.view_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b481f3ad-8eae-4214-ad2f-0986c8f459bf",
   "metadata": {},
   "source": [
    "**TSDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b21f22f-d2e8-41b9-a57f-ca981af6e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Initialize TSDF\n",
    "# =============================================================================\n",
    "args.TSDF_use_mask = True\n",
    "tsdf = TSDF(renderer, stereo, args, strings['TSDF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7d5a9b-1964-445a-be13-744b1047937d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "#  Run TSDF. the TSDF class will have an attribute \"mesh\" with the resulting mesh\n",
    "# ================================================================================\n",
    "%matplotlib inline\n",
    "if not args.skip_TSDF:\n",
    "    tsdf.run(visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94082e81-f79e-446a-8ab5-8f12c60570f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Save the original mesh before cleaning\n",
    "# =============================================================================\n",
    "tsdf.save_mesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68417a-a1f5-43ea-909a-bd2b9a4f3fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Clean the mesh using clustering and save the cleaned mesh.\n",
    "# =============================================================================\n",
    "# original mesh is still available under tsdf.mesh (the cleaned is tsdf.clean_mesh)\n",
    "tsdf.clean_mesh()\n",
    "\n",
    "# saving the cleaned mesh in desired output directory\n",
    "o3d.io.write_triangle_mesh(os.path.join(mesh_output_path, 'template.obj'), tsdf.clean_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a07aaa-7d64-4ea6-bc3b-26ccf537f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Show clean mesh\n",
    "# =============================================================================\n",
    "GT_path = None # OPTIONAL: compare to a GT point cloud if it is aligned with the COLMAP sparse point cloud\n",
    "tsdf.visualize_mesh(subsample=100, GT_path=GT_path, show_clean=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
